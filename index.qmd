---
title: "Addressing the void of AI policies in education for individuals with learning disabilities"
date: "`r Sys.Date()`"
format:
  html:
    fontawesome: true
    code-fold: true
    code-tools: true
    toc: true
---

The website contains supplemental materials and code used to analyze text data in Authors (2025).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


## Set Up

#### Load R Packages

```{r}
suppressPackageStartupMessages({
    library(tidyr)
    library(dplyr)
    library(readxl)
    library(gtsummary)
    library(officer)
    library(flextable)
    library(ggplot2)
    library(quanteda)
    library(stopwords)
    library(TextAnalysisR)
    library(spacyr)
    library(htmlwidgets)
    library(stm)
    library(dotenv)
    library(ggdendro)
    library(plotly)
    library(httr)
    library(jsonlite)
    library(progress)
    library(openxlsx)
})
```

#### Load Dataset

```{r}
#| code-fold: false
data <- read_excel("data/PolicyData.xlsx")
load("data/ai_ld_data.rda")
```

## Preporcess Text Data

#### Convert to a Long Format 

```{r, eval=FALSE}
data <- read_excel("data/PolicyData.xlsx")

data_long <- data |>
  pivot_longer(
    cols           = matches("^coding_\\d+$"),
    names_to       = "coding_id",
    names_prefix   = "coding_",
    values_to      = "coding_text",
    values_drop_na = TRUE
  ) |>
  mutate(
    coding_id   = as.integer(coding_id),
    document_id = paste(ID, coding_id, sep = "_")
 )

united_tbl <- unite_text_cols(data_long, 
                              listed_vars = "coding_text")
```

#### Segment a Corpus Into Tokens

```{r, eval=FALSE}
#| code-fold: false
tokens <- preprocess_texts(united_tbl, text_field = "united_texts", verbose = FALSE)
```

#### Detect Multi-Word Expressions

```{r, eval=FALSE}
#| code-fold: false
tokens_collocations <- detect_multi_word_expressions(tokens, size = 2:5, min_count = 2)
tokens_dict <- quanteda::dictionary(
  list(
    custom = c(
      "access to",
      "ai systems",
      "artificial intelligence",
      "assistive technology",
      "automated systems",
      "best practices",
      "civil rights",
      "decision making",
      "developers should",
      "educational settings",
      "federal government",
      "intelligence systems",
      "learning disability",
      "learning disabilities",
      "machine learning",
      "safety institute",
      "school district",
      "students with disabilities",
      "virtual environments"
    )
  )
)
```

#### Process Tokens With Compound Words

```{r, eval=FALSE}
compound_tokens <- function(tokens, dict) {
  quanteda::tokens_compound(
    tokens,
    pattern = dict,
    concatenator = "_",
    valuetype = "glob",
    window = 0,
    case_insensitive = TRUE,
    join = TRUE,
    keep_unigrams = FALSE,
    verbose = TRUE
  )
}

tokens_compound <- compound_tokens(tokens, tokens_dict)
```

#### Word Frequency

```{r, eval=FALSE}
#| code-fold: false
dfm_object <- dfm(tokens_compound)

word_frequency_plot <- plot_word_frequency(dfm_object, n = 20)
```

```{r, echo=FALSE}
word_frequency_plot
```

#### Remove Predefined Stopwords

```{r, eval=FALSE}
#| code-fold: false
stopwords <- stopwords("en", source = "snowball")
toks_removed <- tokens_remove(tokens_compound, pattern = stopwords, verbose = FALSE)
dfm_init <- dfm(toks_removed)
word_frequency_remove_stopwords <- plot_word_frequency(dfm_init, n = 20)
```

```{r, echo=FALSE}
word_frequency_remove_stopwords
```

#### Remove Common Words in the Dataset

```{r, eval=FALSE}
#| code-fold: false
common_words <- c("na", "may", "can", "[''']s")
toks_removed_common <- tokens_remove(toks_removed, 
                                     pattern = common_words, 
                                     valuetype = "regex", 
                                     verbose = FALSE)
dfm_removed <- dfm(toks_removed_common)
word_frequency_remove_common <- plot_word_frequency(dfm_removed, n = 20)
```

```{r, echo=FALSE}
word_frequency_remove_common
```

#### Lemmatize Tokens

##### Word frequency  

```{r, eval=FALSE}
#| code-fold: false
texts <- sapply(toks_removed_common, paste, collapse = " ")
parsed <- spacy_parse(x = texts, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_lemmatized <- as.tokens(parsed, use_lemma = TRUE)
dfm <- dfm(toks_lemmatized)
word_frequency_lemmatized <- plot_word_frequency(dfm, n = 20)
```

```{r, echo=FALSE}
word_frequency_lemmatized 
```

## Optimal Topic Numbers 

#### Model Diagnostics 

```{r, eval=FALSE}
#| code-fold: false
dfm@docvars$disability <- as.factor(data_long$disability)
dfm@docvars$LD <- as.factor(data_long$LD)

evaluate_optimal_topic_number(
  dfm_object = dfm,
  topic_range = 5:50,
  max.em.its = 75,
  categorical_var = c("disability", "LD"),
  continuous_var = "",
  height = 600,
  width = 800,
  verbose = FALSE)
```

![](images/model_diagnostics.png){fig-align="center" width="700"}

## Structural Topic Modeling

```{r, eval=FALSE}
out <- quanteda::convert(dfm, to = "stm")

topic_model <- stm(
  data = out$meta,
  documents = out$documents,
  vocab = out$vocab,
  max.em.its = 75,
  init.type = "Spectral",
  K = 35,
  prevalence = ~ c("disability", "LD"), 
  verbose = FALSE,
  seed = 1234)

top_topic_terms <- TextAnalysisR::select_top_topic_terms(
  stm_model = topic_model,
  top_term_n = 10,
  verbose = FALSE
)

top_topic_terms %>%
  mutate_if(is.numeric, ~ round(., 3)) %>%
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) %>%
    DT::formatStyle(
      columns = c("topic", "term", "beta"),
      fontSize = '16px'
    )
```

#### Generate Policy Topic Labels Using OpenAI's API

```{r, eval=FALSE}
#| code-fold: false
# Load the Open AI API key from the .env file in the working directory

top_labeled_topic_terms <- TextAnalysisR::generate_survey_items(
  top_labeled_topic_terms ,
  model = "gpt-3.5-turbo",
  temperature = 0.5,
  verbose = FALSE)
```

```{r, echo=FALSE, eval=FALSE}
top_labeled_topic_terms |>
  mutate_if(is.numeric, ~ round(., 3)) |>
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) |>
    DT::formatStyle(
      columns = c("topic", "survey_item", "term", "beta"),
      fontSize = '16px'
    )
```

```{r}
# openxlsx::write.xlsx(top_labeled_topic_terms, "data/top_labeled_topic_terms.xlsx")
```

#### Probability of Words Observed in Each Topic (Beta)

```{r, eval=FALSE}
#| code-fold: false
word_probability_plot <- word_probability_plot(
  top_labeled_topic_terms,
  topic_label = "topic_label",
  ncol = 2,
  height = 5000,
  width = 1500)
```

```{r, echo=FALSE}
word_probability_plot
```

#### Topic Clustering and Dendrogram Visualization

```{r, eval=FALSE}
topic_cor <- stm::topicCorr(topic_model, method = "simple", cutoff = 0.001)

if (nrow(topic_cor$cor) == 0) {
  topic_props <- stm::theta(topic_model)
  
  topic_sim <- topic_props %*% t(topic_props)
  topic_dist <- as.dist(1 - topic_sim)
} else {
  topic_dist <- as.dist(1 - abs(topic_cor$cor))
}

topic_hclust <- hclust(topic_dist, method = "ward.D2")
dend_data <- dendro_data(topic_hclust)

dend_plot <- ggplot() +
  geom_segment(
    data = segment(dend_data),
    aes(x = x, y = y, xend = xend, yend = yend)
  ) +
  geom_text(
    data = label(dend_data),
    aes(x = x, y = y, label = label),
    hjust = -0.1,
    size = 3
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    panel.grid = element_blank()
  ) +
  labs(
    title = "Topic Clustering Dendrogram",
    y = "Distance",
    x = ""
  )

dend_plot <- plotly::ggplotly(dend_plot) %>%
  layout(
    showlegend = FALSE,
    hovermode = "closest",
    width = 800,  
    height = 600  
  )
```

```{r, echo=FALSE}
dend_plot
```

#### Generate Policy Survey Items Using OpenAI's API


```{r, eval=FALSE}
# Reference: TextAnalysisR::generate_topic_labels()

top_labeled_human_loop <- read_excel("data/top_labeled_topic_terms_human_loop.xlsx")

generate_survey_items <- function(top_labeled_human_loop,
                                  topic_cluster_df,
                                  model = "gpt-3.5-turbo",
                                  temperature = 0,
                                  verbose = TRUE) {

  if (file.exists(".env")) {
    dotenv::load_dot_env()
    openai_api_key <- Sys.getenv("OPENAI_API_KEY")
    if (openai_api_key == "") {
      stop("OPENAI_API_KEY not found in .env file")
    }
  } else {
    stop(".env file not found")
  }

  system <- "
You are a highly skilled survey designer specializing in creating Likert-scale survey items for educational policy research. Your task is to generate clear, concise survey items that assess the importance of various AI policy aspects for students with learning disabilities.

Guidelines for generating survey items:

1. Format and Structure
   - Create statements that can be rated on a 5-point Likert scale (1=Strongly Disagree to 5=Strongly Agree)
   - Keep items clear, concise, and focused on a single concept
   - Use active voice and present tense
   - Avoid double-barreled questions or complex statements

2. Content Focus
   - Focus on the importance or necessity of the policy aspect
   - Use person-first language (e.g., 'students with learning disabilities' instead of 'disabled students')
   - Ensure items are specific and actionable
   - Avoid jargon unless necessary for technical accuracy

3. Clarity and Precision
   - Use simple, direct language
   - Avoid ambiguous terms or complex sentence structures
   - Make sure each item measures exactly one concept
   - Ensure items are relevant to educational settings

4. Response Scale Considerations
   - Frame items to work well with a 5-point Likert scale
   - Avoid extreme language that might force responses to one end of the scale
   - Ensure items can be meaningfully rated on the agree-disagree continuum

Example
----------
Top Terms (highest to lowest beta score):
virtual manipulatives (.035)
manipulatives (.022)
mathematical (.014)
app (.013)
solving (.013)
learning disability (.012)
algebra (.012)
area (.011)
tool (.010)
concrete manipulatives (.010)

Generated Survey Item: 
AI-powered visual manipulatives should be made available to support mathematical problem-solving for students with learning disabilities.

Focus on creating clear, measurable items that capture the essence of each topic while following these guidelines.
"
  
  top_topic_terms <- top_labeled_human_loop |>
    dplyr::group_by(topic) %>%
    dplyr::arrange(desc(beta)) %>%
    dplyr::ungroup()

  unique_topics <- top_topic_terms %>%
    dplyr::distinct(topic) %>%
    dplyr::arrange(as.numeric(topic)) %>%
    dplyr::mutate(topic = row_number(), survey_item = NA)

  if (verbose) {
    if (!requireNamespace("progress", quietly = TRUE)) {
      utils::install.packages("progress")
    }
    pb <- progress::progress_bar$new(
      format = " Processing [:bar] :percent ETA: :eta",
      total = nrow(unique_topics),
      clear = FALSE, width = 60 )
  }

  for (i in seq_len(nrow(unique_topics))) {
    if (verbose) {
      pb$tick()
    }

    current_topic <- unique_topics$topic[i]

    selected_terms <- top_topic_terms %>%
      dplyr::filter(topic == current_topic) %>%
      dplyr::pull(term)

    user <- paste0(
      "You have a topic with keywords listed from most to least significant:",
      paste(selected_terms, collapse = ", "), "\n\n",
      "Please create a single survey item that:\n",
      "1. Captures the essence of this topic\n",
      "2. Can be rated on a 5-point Likert scale (1=Strongly Disagree to 5=Strongly Agree)\n",
      "3. Focuses on the importance of this policy aspect\n",
      "4. Uses person-first language\n",
      "5. Is clear, concise, and specific"
    )

     body_list <- list(
      model = model,
      messages = list(
        list(role = "system", content = system),
        list(role = "user", content = user)
      ),
      temperature = temperature,
      max_tokens = 50
    )

    response <- httr::POST(
      url = "https://api.openai.com/v1/chat/completions",
      httr::add_headers(
        `Content-Type` = "application/json",
        `Authorization` = paste("Bearer", openai_api_key)
      ),
      body = jsonlite::toJSON(body_list, auto_unbox = TRUE),
      encode = "json"
    )

    if (httr::status_code(response) != 200) {
      warning(sprintf("OpenAI API request failed for topic '%s': %s",
                      current_topic, httr::content(response, "text", encoding = "UTF-8")))
      next
    }

    res_json <- jsonlite::fromJSON(httr::content(response, "text", encoding = "UTF-8"))

    if (verbose) {
      cat("Response JSON structure for topic", i, ":\n")
      print(str(res_json))
    }

    if (!is.null(res_json$choices) && nrow(res_json$choices) > 0) {
      if (!is.null(res_json$choices$message$content)) {
        survey_item <- res_json$choices$message$content[1]
        survey_item <- trimws(survey_item)
        survey_item <- gsub('^"(.*)"$', '\\1', survey_item)
        unique_topics$survey_item[i] <- survey_item
      } else {
        warning(sprintf("Unexpected response structure for topic '%s': %s", current_topic, jsonlite::toJSON(res_json, auto_unbox = TRUE)))
        next
      }
    } else {
      warning(sprintf("Unexpected response structure for topic '%s': %s", current_topic, jsonlite::toJSON(res_json, auto_unbox = TRUE)))
      next
    }

    Sys.sleep(1)
  }

  top_labeled_topic_terms <- top_topic_terms %>%
    dplyr::left_join(unique_topics, by = "topic") %>%
    dplyr::select(survey_item, topic, term, beta) %>%
    dplyr::arrange(topic, desc(beta))

  return(top_labeled_topic_terms)
}
```

```{r, eval=FALSE}
#| code-fold: false
policy_survey_items <- generate_survey_items(
    top_labeled_human_loop,
    topic_cluster_df,
    model = "gpt-3.5-turbo",
    temperature = 0,
    verbose = TRUE)
```

```{r, echo=FALSE}
policy_survey_items_dist <- 
    policy_survey_items %>%
    group_by(topic) %>%
    summarise(
        survey_item = first(survey_item),
        term_beta = paste(term, "(", round(beta, 3), ")", collapse = ", ")
    ) %>%
    ungroup()

policy_survey_items_dist %>%
    DT::datatable(
        rownames = FALSE,
        extensions = 'Buttons',
        options = list(
            scrollX = TRUE,
            scrollY = "400px",
            width = "80%",
            dom = 'Bfrtip',
            buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
        )
    ) %>%
    DT::formatStyle(
        columns = c("topic", "survey_item", "term_beta"),
        fontSize = '16px'
    )

openxlsx::write.xlsx(policy_survey_items_dist, "data/policy_survey_items.xlsx")
```

```{r, echo=FALSE, eval=FALSE}
save(dfm, word_frequency_plot, word_frequency_remove_stopwords, word_frequency_remove_common,
     word_frequency_lemmatized, toks_removed_common, toks_removed, toks_lemmatized,
     dfm_object, dfm_removed, dfm_init, tokens_compound, tokens_collocations,
     tokens_dict, tokens, united_tbl, data_long, 
     dend_plot, topic_hclust, topic_dist, topic_cor,
     topic_cluster_df,
     policy_survey_items,
     policy_survey_items_dist,
     out, top_labeled_topic_terms, word_probability_plot, topic_model, file = "data/ai_ld_data.rda")
```
---
title: "Addressing the void of AI policies in education for individuals with learning disabilities"
date: "`r Sys.Date()`"
format:
  html:
    fontawesome: true
    code-fold: true
    code-tools: true
    toc: true
---

The website contains supplemental materials and code used to analyze text data in Authors (2025).

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


## Set Up

#### Load R Packages

```{r}
suppressPackageStartupMessages({
    library(tidyr)
    library(dplyr)
    library(readxl)
    library(brms)
    library(gtsummary)
    library(officer)
    library(flextable)
    library(ggplot2)
    library(quanteda)
    library(stopwords)
    library(TextAnalysisR)
    library(spacyr)
    library(htmlwidgets)
    library(stm)
})
```

#### Load Dataset

```{r}
#| code-fold: false
data <- read_excel("data/PolicyData.xlsx")
# load("data/ai_ld_data.rda")
```

## Preporcess Text Data

#### Convert to a Long Format 

```{r}
#| code-fold: false
data <- read_excel("data/PolicyData.xlsx")

data_long <- data %>%
  pivot_longer(
    cols           = matches("^coding_\\d+$"),
    names_to       = "coding_id",
    names_prefix   = "coding_",
    values_to      = "coding_text",
    values_drop_na = TRUE
  ) %>%
  mutate(
    coding_id   = as.integer(coding_id),
    document_id = paste(ID, coding_id, sep = "_")
 )

united_tbl <- unite_text_cols(data_long, 
                              listed_vars = "coding_text")
```

#### Segment a Corpus Into Tokens

```{r}
#| code-fold: false
tokens <- preprocess_texts(united_tbl, text_field = "united_texts", verbose = FALSE)
```

#### Detect Multi-Word Expressions

```{r}
#| code-fold: false
tokens_collocations <- detect_multi_word_expressions(tokens, size = 2:5, min_count = 2)
tokens_dict <- quanteda::dictionary(
  list(
    custom = c(
      "access to",
      "ai systems",
      "artificial intelligence",
      "assistive technology",
      "automated systems",
      "best practices",
      "civil rights",
      "decision making",
      "developers should",
      "educational settings",
      "federal government",
      "intelligence systems",
      "learning disability",
      "learning disabilities",
      "machine learning",
      "safety institute",
      "school district",
      "students with disabilities",
      "virtual environments"
    )
  )
)
```

#### Process Tokens With Compound Words

```{r}
#| code-fold: false
compound_tokens <- function(tokens, dict) {
  quanteda::tokens_compound(
    tokens,
    pattern = dict,
    concatenator = "_",
    valuetype = "glob",
    window = 0,
    case_insensitive = TRUE,
    join = TRUE,
    keep_unigrams = FALSE,
    verbose = TRUE
  )
}

tokens_compound <- compound_tokens(tokens, tokens_dict)
```

#### Word Frequency

```{r}
#| code-fold: false
dfm_object <- dfm(tokens_compound)

word_frequency_plot <- plot_word_frequency(dfm_object, n = 20)
```

```{r, echo=FALSE}
word_frequency_plot
```

#### Remove Predefined Stopwords

```{r}
#| code-fold: false
stopwords <- stopwords("en", source = "snowball")
toks_removed <- tokens_remove(tokens_compound, pattern = stopwords, verbose = FALSE)
dfm_init <- dfm(toks_removed)
word_frequency_remove_stopwords <- plot_word_frequency(dfm_init, n = 40)
```

```{r, echo=FALSE}
word_frequency_remove_stopwords
```

#### Remove Common Words in the Dataset

```{r}
#| code-fold: false
common_words <- c("na", "may", "can", "[''']s")
toks_removed_common <- tokens_remove(toks_removed, 
                                     pattern = common_words, 
                                     valuetype = "regex", 
                                     verbose = FALSE)
dfm_removed <- dfm(toks_removed_common)
word_frequency_remove_common <- plot_word_frequency(dfm_removed, n = 20)
```

```{r, echo=FALSE}
word_frequency_remove_common
```

#### Lemmatize Tokens

##### Word frequency  

```{r}
#| code-fold: false
texts <- sapply(toks_removed_common, paste, collapse = " ")
parsed <- spacy_parse(x = texts, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_lemmatized <- as.tokens(parsed, use_lemma = TRUE)
dfm <- dfm(toks_lemmatized)
word_frequency_lemmatized <- plot_word_frequency(dfm, n = 20)
```

```{r, echo=FALSE}
word_frequency_lemmatized 
```

## Optimal Topic Numbers 

#### Model Diagnostics 

```{r, eval=FALSE}
#| code-fold: false
dfm@docvars$disability <- as.factor(data_long$disability)
dfm@docvars$LD <- as.factor(data_long$LD)

evaluate_optimal_topic_number(
  dfm_object = dfm,
  topic_range = 5:50,
  max.em.its = 75,
  categorical_var = c("disability", "LD"),
  continuous_var = "",
  height = 600,
  width = 800,
  verbose = FALSE)
```

![](images/model_diagnostics.png){fig-align="center" width="700"}

## Structural Topic Modeling

```{r}
#| code-fold: false
out <- quanteda::convert(dfm, to = "stm")

topic_model <- stm(
  data = out$meta,
  documents = out$documents,
  vocab = out$vocab,
  max.em.its = 75,
  init.type = "Spectral",
  K = 35,
  prevalence = ~ c("disability", "LD"), 
  verbose = FALSE,
  seed = 1234)

top_topic_terms <- TextAnalysisR::select_top_topic_terms(
  stm_model = topic_model,
  top_term_n = 10,
  verbose = FALSE
)

top_topic_terms %>%
  mutate_if(is.numeric, ~ round(., 3)) %>%
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) %>%
    DT::formatStyle(
      columns = c("topic", "term", "beta"),
      fontSize = '16px'
    )
```

#### Generate Policy Topic Labels Using OpenAI's API

```{r, eval=FALSE}
#| code-fold: false
# Load the Open AI API key from the .env file in the working directory

top_labeled_topic_terms <- generate_topic_labels(
  top_topic_terms,
  model = "gpt-3.5-turbo",
  temperature = 0.5,
  verbose = FALSE)
```

```{r, echo=FALSE}
top_labeled_topic_terms %>%
  mutate_if(is.numeric, ~ round(., 3)) %>%
      DT::datatable(
      rownames = FALSE,
      extensions = 'Buttons',
      options = list(
        scrollX = TRUE,
        scrollY = "400px",
        width = "80%",
        dom = 'Bfrtip',
        buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
      )
    ) %>%
    DT::formatStyle(
      columns = c("topic", "topic_label", "term", "beta"),
      fontSize = '16px'
    )
```

```{r}
openxlsx::write.xlsx(top_labeled_topic_terms, "data/top_labeled_topic_terms.xlsx")
```

#### Probability of Words Observed in Each Topic (Beta)

```{r}
#| code-fold: false
word_probability_plot <- word_probability_plot(
  top_labeled_topic_terms,
  topic_label = "topic_label",
  ncol = 2,
  height = 5000,
  width = 1500)
```

```{r, echo=FALSE}
word_probability_plot
```

#### Topic Clustering and Dendrogram Visualization

```{r}
#| code-fold: false
# Calculate topic correlation matrix
topic_cor <- stm::topicCorr(topic_model, method = "simple", cutoff = 0.01)

# Convert correlation matrix to distance matrix
topic_dist <- as.dist(1 - abs(topic_cor$cor))

# Perform hierarchical clustering using Ward's method
topic_hclust <- hclust(topic_dist, method = "ward.D2")

# Create dendrogram plot using plotly
library(plotly)

# Create dendrogram data
dend_data <- dendro_data(topic_hclust)

# Create plotly dendrogram
dend_plot <- plot_ly() %>%
  add_segments(
    data = segment(dend_data),
    x = ~x, y = ~y,
    xend = ~xend, yend = ~yend
  ) %>%
  add_text(
    data = label(dend_data),
    x = ~x, y = ~y,
    text = ~label,
    textposition = "middle right",
    hoverinfo = "text"
  ) %>%
  layout(
    title = "Topic Clustering Dendrogram",
    xaxis = list(
      title = "",
      showticklabels = FALSE,
      zeroline = FALSE
    ),
    yaxis = list(
      title = "Distance",
      zeroline = FALSE
    ),
    showlegend = FALSE
  )

# Display the plot
dend_plot

# Cut the dendrogram to create clusters
num_clusters <- 5  # Adjust this number based on your needs
topic_clusters <- cutree(topic_hclust, k = num_clusters)

# Create a data frame with topic clusters
topic_cluster_df <- data.frame(
  topic = names(topic_clusters),
  cluster = topic_clusters
) %>%
  arrange(cluster, topic)

# Save topic clusters for later use
write.csv(topic_cluster_df, "data/topic_clusters.csv", row.names = FALSE)
```

#### Generate Policy Survey Items Based on Topic Modeling Results Using OpenAI's API

```{r}
generate_survey_items <- function(top_topic_terms,
                                model = "gpt-3.5-turbo",
                                system = NULL,
                                user = NULL,
                                temperature = 0.5,
                                openai_api_key = NULL,
                                verbose = TRUE) {

  if (!requireNamespace("dotenv", quietly = TRUE) ||
      !requireNamespace("httr", quietly = TRUE) ||
      !requireNamespace("jsonlite", quietly = TRUE)) {
    stop(
      "The 'dotenv', 'httr', and 'jsonlite' packages are required for this functionality. ",
      "Please install them using install.packages(c('dotenv', 'httr', 'jsonlite'))."
    )
  }

  if (file.exists(".env")) {
    dotenv::load_dot_env()
  }

  if (is.null(openai_api_key)) {
    openai_api_key <- Sys.getenv("OPENAI_API_KEY")
  }

  if (nzchar(openai_api_key) == FALSE) {
    stop("No OpenAI API key provided or found in OPENAI_API_KEY environment variable.")
  }

  system <- "
You are a highly skilled survey designer specializing in creating Likert-scale survey items for educational policy research.
Your task is to generate clear, concise survey items that assess the importance of various AI policy aspects for students with learning disabilities.

Guidelines for creating survey items:

1. Format and Structure
   - Create statements that can be rated on a 5-point Likert scale (1=Strongly Disagree to 5=Strongly Agree)
   - Keep items clear, concise, and focused on a single concept
   - Use active voice and present tense
   - Avoid double-barreled questions or complex statements

2. Content Focus
   - Focus on the importance or necessity of the policy aspect
   - Use person-first language (e.g., 'students with learning disabilities' instead of 'disabled students')
   - Ensure items are specific and actionable
   - Avoid jargon unless necessary for technical accuracy

3. Clarity and Precision
   - Use simple, direct language
   - Avoid ambiguous terms or complex sentence structures
   - Make sure each item measures exactly one concept
   - Ensure items are relevant to educational settings

4. Response Scale Considerations
   - Frame items to work well with a 5-point Likert scale
   - Avoid extreme language that might force responses to one end of the scale
   - Ensure items can be meaningfully rated on the agree-disagree continuum

Example:
Topic: Visual-based technology for mathematical problem solving
Generated Survey Item: 'AI-powered visual manipulatives should be made available to support mathematical problem-solving for students with learning disabilities.'

Focus on creating clear, measurable items that capture the essence of each topic while following these guidelines.
"

  # Load topic clusters
  topic_clusters <- read.csv("data/topic_clusters.csv")
  
  top_topic_terms <- top_topic_terms %>%
    dplyr::group_by(topic) %>%
    dplyr::arrange(desc(beta)) %>%
    dplyr::ungroup()

  unique_topics <- top_topic_terms %>%
    dplyr::distinct(topic) %>%
    dplyr::arrange(as.numeric(topic)) %>%
    dplyr::mutate(topic = row_number(), survey_item = NA)

  if (verbose) {
    if (!requireNamespace("progress", quietly = TRUE)) {
      utils::install.packages("progress")
    }
    pb <- progress::progress_bar$new(
      format = " Processing [:bar] :percent ETA: :eta",
      total = nrow(unique_topics),
      clear = FALSE, width = 60 )
  }

  for (i in seq_len(nrow(unique_topics))) {
    if (verbose) {
      pb$tick()
    }

    current_topic <- unique_topics$topic[i]
    
    # Get topic label and cluster
    topic_info <- top_labeled_topic_terms %>%
      dplyr::filter(topic == current_topic) %>%
      dplyr::distinct(topic_label) %>%
      dplyr::left_join(topic_clusters, by = "topic")

    selected_terms <- top_topic_terms %>%
      dplyr::filter(topic == current_topic) %>%
      dplyr::pull(term)

    user <- paste0(
      "Topic Label: ", topic_info$topic_label, "\n",
      "Cluster: ", topic_info$cluster, "\n",
      "Top Terms (highest to lowest beta score): ",
      paste(selected_terms, collapse = ", "), "\n\n",
      "Please create a single survey item that:",
      "1. Captures the essence of this topic",
      "2. Can be rated on a 5-point Likert scale (1=Strongly Disagree to 5=Strongly Agree)",
      "3. Focuses on the importance of this policy aspect",
      "4. Uses person-first language",
      "5. Is clear, concise, and specific"
    )

    body_list <- list(
      model = model,
      messages = list(
        list(role = "system", content = system),
        list(role = "user", content = user)
      ),
      temperature = temperature,
      max_tokens = 100
    )

    response <- httr::POST(
      url = "https://api.openai.com/v1/chat/completions",
      httr::add_headers(
        `Content-Type` = "application/json",
        `Authorization` = paste("Bearer", openai_api_key)
      ),
      body = jsonlite::toJSON(body_list, auto_unbox = TRUE),
      encode = "json"
    )

    if (httr::status_code(response) != 200) {
      warning(sprintf("OpenAI API request failed for topic '%s': %s",
                      current_topic, httr::content(response, "text", encoding = "UTF-8")))
      next
    }

    res_json <- jsonlite::fromJSON(httr::content(response, "text", encoding = "UTF-8"))

    if (!is.null(res_json$choices) && nrow(res_json$choices) > 0) {
      if (!is.null(res_json$choices$message$content)) {
        survey_item <- res_json$choices$message$content[1]
        survey_item <- trimws(survey_item)
        survey_item <- gsub('^"(.*)"$', '\\1', survey_item)
        unique_topics$survey_item[i] <- survey_item
      } else {
        warning(sprintf("Unexpected response structure for topic '%s': %s", current_topic, jsonlite::toJSON(res_json, auto_unbox = TRUE)))
        next
      }
    } else {
      warning(sprintf("Unexpected response structure for topic '%s': %s", current_topic, jsonlite::toJSON(res_json, auto_unbox = TRUE)))
      next
    }

    Sys.sleep(1)
  }

  # Create final output with clusters
  survey_items <- unique_topics %>%
    dplyr::left_join(topic_clusters, by = "topic") %>%
    dplyr::left_join(
      top_labeled_topic_terms %>%
        dplyr::distinct(topic, topic_label),
      by = "topic"
    ) %>%
    dplyr::select(cluster, topic, topic_label, survey_item) %>%
    dplyr::arrange(cluster, topic)

  # Save to Excel
  openxlsx::write.xlsx(survey_items, "data/survey_items.xlsx")

  return(survey_items)
}
```


```{r, echo=FALSE}
save(dfm, word_frequency_plot, word_frequency_remove_stopwords, word_frequency_remove_common,
     word_frequency_lemmatized, toks_removed_common, toks_removed, toks_lemmatized,
     out, top_labeled_topic_terms, word_probability_plot, topic_model, file = "data/ai_ld_data.rda")
```





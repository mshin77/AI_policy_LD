---
title: "Code"
date: ""
format:
  html:
    code-fold: true
    code-tools: true
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)
```


## Set Up

#### Install R Packages

```{r}
#| code-fold: false
# suppressWarnings({
# install.packages("readxl", "tidyverse", "dplyr", "tidytext", "stringr", "ggplot2", "plotly", "stm", "DT", "purrr", "furrr", "future", "tidyr", "reshape2", "stminsights", "numform")
# devtools::install_github("mshin77/TextAnalysisR")
# })
```

#### Load R Packages

```{r}
suppressPackageStartupMessages({
    library(tidyr)
    library(dplyr)
    library(readxl)
    library(brms)
    library(gtsummary)
    library(officer)
    library(flextable)
    library(ggplot2)
    library(quanteda)
    library(stopwords)
    library(TextAnalysisR)
    library(spacyr)
    library(htmlwidgets)
})

suppressWarnings(library(tidyverse))
```

#### Load Dataset

```{r}
#| code-fold: false
data <- read_excel("data/PolicyData.xlsx")
# load("data/ai_ld_data.RData")
```

## Preporcess Text Data

#### Unite Text Columns

```{r, eval=FALSE}
#| code-fold: false
united_tbl <- unite_text_cols(data, 
                              listed_vars = paste0("Text", 1:22))
                              
united_tbl <- united_tbl %>% 
   dplyr::select(-all_of(paste0("Text", 1:22)))
                              
openxlsx::write.xlsx(united_tbl, "data/united_tbl.xlsx")                             
```

#### Segment a Corpus Into Tokens

```{r, eval=FALSE}
#| code-fold: false
tokens <- preprocess_texts(united_tbl, text_field = "united_texts", verbose = FALSE)
```

#### Detect Multi-Word Expressions

```{r, eval=FALSE}
#| code-fold: false
tokens_collocations <- detect_multi_word_expressions(tokens, size = 2:5, min_count = 2)
tokens_dict <- quanteda::dictionary(
list(custom = 
c("intelligence systems", 
"artificial intelligence", 
"civil rights",
"federal government",
"automated systems",
"decision making")))

```

#### Process Tokens With Compound Words

```{r, eval=FALSE}
#| code-fold: false
compound_tokens <- function(tokens, dict) {
  quanteda::tokens_compound(
    tokens,
    pattern = dict,
    concatenator = "_",
    valuetype = "glob",
    window = 0,
    case_insensitive = TRUE,
    join = TRUE,
    keep_unigrams = FALSE,
    verbose = TRUE
  )
}

tokens_compound <- compound_tokens(tokens, tokens_dict)
```

#### Word Frequency

```{r, eval=FALSE}
#| code-fold: false
dfm_object <- dfm(tokens_compound)

word_frequency_plot <- plot_word_frequency(dfm_object, n = 20)
```

#### Remove Predefined Stopwords

```{r, eval=FALSE}
#| code-fold: false
stopwords <- stopwords("en", source = "snowball")
toks_removed <- tokens_remove(tokens_compound, pattern = stopwords, verbose = FALSE)
dfm_init <- dfm(toks_removed)
plot_word_frequency(dfm_init, n = 40)
```

#### Remove Common Words in the Dataset

```{r, eval=FALSE}
#| code-fold: false
common_words <- c("na", "may", "can", "['â€™]s")
toks_removed_common <- tokens_remove(toks_removed, 
                                     pattern = common_words, 
                                     valuetype = "regex", 
                                     verbose = FALSE)
dfm_removed <- dfm(toks_removed_common)
plot_word_frequency(dfm_removed, n = 20)
```

#### Lemmatize Tokens

##### Word frequency  

```{r, eval=FALSE}
#| code-fold: false
texts <- sapply(toks_removed_common, paste, collapse = " ")
parsed <- spacy_parse(x = texts, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_lemmatized <- as.tokens(parsed, use_lemma = TRUE)
dfm <- dfm(toks_lemmatized)
dfm_freq <- plot_word_frequency(dfm, n = 20)
```

```{r, echo=FALSE}
dfm_freq
```

## Identification of Optimal Topic Numbers 

#### Model Diagnostics 

```{r, eval = FALSE}

dfm@docvars$Disability <- as.factor(data$Disability)
dfm@docvars$LD <- as.factor(data$LD)

evaluate_optimal_topic_number(
  dfm_object = dfm,
  topic_range = 5:30,
  max.em.its = 75,
  categorical_var = c("Disability", "LD"),
  continuous_var = "",
  height = 600,
  width = 800,
  verbose = TRUE)
```

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
K_result %>%
  transmute(K,
            `Lower bound` = lbound,
            Residuals = map_dbl(residual, "dispersion"),
            `Semantic coherence` = map_dbl(semantic_coherence, mean),
            `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")) %>%
  gather(Metric, Value, -K) %>%
  ggplot(aes(K, Value, color = Metric)) +
  theme_bw() +
  geom_line(size = 1, alpha = 0.7, show.legend = FALSE) +
  geom_point(size = 3, alpha = 0.7, show.legend = FALSE) +
  theme_bw() +
  theme(
    axis.line = element_line(color = "#3B3B3B", linewidth = 0.3),
    axis.ticks = element_line(color = "#3B3B3B", linewidth = 0.3),
    strip.text.x = element_text(size = 11, color = "#3B3B3B"), 
    axis.text.x = element_text(size = 11, color = "#3B3B3B"),
    axis.text.y = element_text(size = 11, color = "#3B3B3B"),
    axis.title = element_text(size = 11, color = "#3B3B3B"),
    axis.title.x = element_text(size = 12, margin = margin(t = 15)),
    axis.title.y = element_text(size = 12, margin = margin(r = 10))) +
  facet_wrap(~Metric, scales = "free_y") +
  labs(x = "Number of Topics (K)",
       y = NULL,
       title = "")
```

#### 3.2. Structural Topic Modeling

```{r, eval = FALSE}
#| code-fold: false
topic_model <- stm(dfm, K = 6, 
                   prevalence = ~ policy_topic, 
                   max.em.its = 75, 
                   init.type = "Spectral", 
                   verbose = FALSE, 
                   seed = 1234)
```

## 4. Correlations Between Topics (Research Question 2)

```{r, fig.width = 8, fig.height = 5}
extract_lower_tri <- function(x){
  x[upper.tri(x)] <- NA
  return(x)
}

corr <- topicCorr(topic_model) %>% .$cor %>% 
  extract_lower_tri() %>% 
  melt(na.rm = T) %>% 
  ggplot(aes(x = factor(Var1), 
             y = factor(Var2), 
             fill = value)) +
  geom_tile(color = "white") + 
  scale_fill_gradient2(name = "Value", 
                       low = "#1f459c", high = "#94465a", mid = "white",
                       midpoint = 0,
                       limit = c(-1, 1), space = "Lab") +
  geom_text(aes(Var1, Var2, label = round(value, 3)), color = "black", size = 3.5) +
  labs(x = "Topic", y = "Topic",
       title = "") +
  theme_bw() +
  theme(
    axis.line = element_line(color = "#3B3B3B", linewidth = 0.3),
    axis.ticks = element_line(color = "#3B3B3B", linewidth = 0.3),
    strip.text.x = element_text(size = 11, color = "#3B3B3B"),
    axis.text.x = element_text(size = 11, color = "#3B3B3B"),  
    axis.text.y = element_text(size = 11, color = "#3B3B3B"),  
    axis.title = element_text(size = 11, color = "#3B3B3B"),
    axis.title.x = element_text(size = 12, margin = margin(t = 15)),
    axis.title.y = element_text(size = 12, margin = margin(r = 10)),
    legend.title = element_text(size = 11), 
    legend.text = element_text(size = 11))
```

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
corr
```

## 5. Education Policies and Practices for Students With LD (Research Question 3)

#### 5.1. Probability of Words Observed in Each Topic (Beta)

```{r, eval = FALSE}
tidy_beta <- tidy(topic_model, 
                  matrix = "beta", 
                  document_names = rownames(dfm), 
                  log = FALSE) 

tidy_beta_table <- tidy_beta %>% 
  mutate(beta = round(beta, 3)) %>%
  arrange(topic, desc(beta), .by_group = TRUE) 
```

```{r, echo = FALSE}
datatable(tidy_beta_table, rownames = FALSE)
```

#### 5.2. Top 10 Associated Words per Topic

```{r, fig.width = 9, fig.height = 8}
top_term <- tidy_beta %>% 
    examine_top_terms(top_n = 10)  

top_term$term <- c("reinforcement", "development", "management", "teacher", 
                   "education", "education", "Ministry of Education", 
                   "Education Office", "composition", "establishment", 
                   "institution", "function", "foundation", "basic education", 
                   "slow learners", "diverse", "responsible", "target", "measures", 
                   "legislation", "case", "work", "capacity", "role", "connection", 
                   "budget", "operation", "recognition", "data", "disability", 
                   "proactive", "expert", "expertise", "information", "policy", "provide", 
                   "focus", "intelligence", "support", "support", "support", "diagnosis", 
                   "diagnosis", "career", "inclusive classroom", "special education teacher", 
                   "special education", "students eligible for special education", 
                   "special education support center", "special class", "lifelong education", 
                   "lifelong education institution", "program", "school", "parents", "student", 
                   "learning", "learning disability", "cooperation", "promotion")

top_term %>% plot_topic_term(ncol = 2) +
    labs(x = "Word", y = expression("Word Probabilities"~(beta))) +
      theme(axis.text.y = element_text(size = 13, color = "#3B3B3B"))
```

#### 5.3. Probability of Topics per Document (Gamma)

```{r, eval = FALSE}
tidy_gamma <- tidy(topic_model,
                   matrix = "gamma",
                   document_names = rownames(dfm), log = FALSE) 

tidy_gamma_table <- tidy_gamma %>% 
  mutate(gamma= round(gamma, 3)) %>%
  arrange(topic, desc(gamma), .by_group = TRUE) 
```

```{r, echo = FALSE}
datatable(tidy_gamma_table, rownames = FALSE)
```

#### 5.4. Topic Proportions in the Whole Corpus of Policy Items

```{r, fig.width = 8, fig.height = 5, echo = FALSE}
tidy_gamma_gg <- tidy_gamma %>%
    topic_probability_plot(top_n = 15) %>% plotly::ggplotly()

tidy_gamma_gg %>% layout(title = "")
```

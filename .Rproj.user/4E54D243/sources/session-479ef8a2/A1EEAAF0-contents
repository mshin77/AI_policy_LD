---
title: "Assessing the Social Validity of CANVASÂ® LMS-Based Cybersecurity Online Modules in High School STEM Classes"
date: "`r Sys.Date()`"
format:
  html:
    code-fold: true
    code-tools: true
    self-contained: false
    fontawesome: true
    navbar:
      right:
        - icon: github
          href: 
---

```{=html}
<style>
  .responsive-figure {
    width: 100% !important;
    height: auto;
  }
</style>
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE)
```

The website contains R code used to analyze numeric, ordinal, and text data in Hossain et al. (2025). Reproducible materials are also posted at the [Center for Open Science](https://osf.io/byrw7/?view_only=ec0b6ba61fef4134b371ff54fead3314).

::: panel-tabset

## Participants

#### Preprocessing

```{r}
#| code-fold: false
suppressPackageStartupMessages({
    library(tidyr)
    library(dplyr)
    library(readxl)
    library(brms)
    library(gtsummary)
    library(officer)
    library(flextable)
    library(ggplot2)
    library(quanteda)
    library(stopwords)
    library(TextAnalysisR)
    library(spacyr)
    library(htmlwidgets)
})

# student_data <- read_excel("data/student_data.xlsx")

load("data/cybersecurity_clmm.RData")
load("data/cybersecurity_network.RData")
```

#### Demographic Profiles of Participants

```{r, eval=FALSE, echo=FALSE}
demo_tbl <- student_data %>%
  dplyr::select(mobile_time, mobile_school, mobile_home, grade, gender, race) %>%
  tbl_summary(
              percent = "column") 
  
demo_tbl_ft <- as_flex_table(demo_tbl)

read_docx() %>%
  body_add_flextable(demo_tbl_ft) %>%
  print(target = "data/demo_tbl_ft.docx")
```

```{r}
demo_tbl_ft
```

## Priors

```{r, eval=FALSE}
#| code-fold: false
long_data <- student_data %>%
    pivot_longer(
        cols = starts_with("social_validity_"),
        names_to = "item",
        names_pattern = "social_validity_(.*)",
        values_to = "rating"
    )

long_data$rating <- factor(long_data$rating, ordered = TRUE)
long_data$tech_navigating_using <- factor(long_data$tech_navigating_using, ordered = TRUE)
long_data$tech_learning_sequence <- factor(long_data$tech_learning_sequence, ordered = TRUE)
long_data$tech_self_regulation <- factor(long_data$tech_self_regulation, ordered = TRUE)
```

##### Check Priors

```{r, eval=FALSE}
#| code-fold: false
formula <- rating ~ 
    mo(tech_navigating_using) + 
    mo(tech_learning_sequence) + 
    mo(tech_self_regulation) + (1 | item) + (1 | student)

priors <- get_prior(
  formula = formula,
  data = long_data,
  family = cumulative("logit")
)

priors
```

## Bayesian CLMM 

##### Fit the Bayesian Cumulative Link Mixed Effects Model (CLMM) With Priors

```{r, eval=FALSE}
#| code-fold: false
model <- brm(
  formula = formula,
  data = long_data,
  family = cumulative("logit"),
  prior = priors,
  iter = 3000,
  warmup = 1000,
  chains = 4,
  cores = 4,
  thin = 4,
  seed = 2025,
  control = list(adapt_delta = 0.95, max_treedepth = 15)
)
```

##### Posterior Predictive Check

```{r}
pp_check_plot <- pp_check(model, type = "bars") +
    theme_classic() +
    theme(
        text = element_text(size = 12),
        # panel.grid.major = element_line(color = "gray80"),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank(),
        axis.title = element_text(size = 12),
        axis.text = element_text(size = 12),
        legend.text = element_text(size = 12),
        legend.title = element_text(size = 12),
        plot.title = element_text(size = 12, face = "bold")
    ) 
pp_check_plot
```


```{r, eval=FALSE, echo=FALSE}
ggsave("data/pp_check_plot.png", pp_check_plot, width = 6, height = 4)
```

```{r}
summary(model)
```

#### Random Effect for Items

```{r}
#| code-fold: false
Var <- VarCorr(model)

var_item <- Var$item$sd[1]^2
var_item
```

#### Random Effect for Students 

```{r}
#| code-fold: false
var_students <- Var$student$sd[1]^2
var_students
```

##### Fixed Coefficients in Logit

```{r}
#| code-fold: false
fixed_coefficient <- fixef(model)

# Convert to Odds Ratio
fixed_coefficient %>% data.frame() %>% 
    mutate(OR = exp(Estimate)) %>% 
    mutate(OR_Q2.5 = exp(Q2.5)) %>%
    mutate(OR_Q97.5 = exp(Q97.5)) %>%
    mutate_if(is.numeric, ~ round(., 2))
```

## Preprocess Text

#### Unite Text Columns

```{r, eval=FALSE}
#| code-fold: false
united_tbl_most <- unite_text_cols(student_data, 
                              listed_vars = c("most_interesting_unit", 
                                              "most_like_topic"))

united_tbl_least <- unite_text_cols(student_data, 
                              listed_vars = c("least_interesting_unit",
                                              "least_like_topic"))

united_tbl_overall <- unite_text_cols(student_data, 
                              listed_vars = c(
                                              "future_learning",
                                              "share_thoughts",
                                              "overall_experience"))
```

#### Segment a Corpus Into Tokens

```{r, eval=FALSE}
#| code-fold: false
tokens_most <- preprocess_texts(united_tbl_most, text_field = "united_texts", verbose = FALSE)
tokens_least <- preprocess_texts(united_tbl_least, text_field = "united_texts", verbose = FALSE)
tokens_overall <- preprocess_texts(united_tbl_overall, text_field = "united_texts", verbose = FALSE)
```

#### Detect Multi-Word Expressions

```{r, eval=FALSE}
#| code-fold: false
tokens_most_collocations <- detect_multi_word_expressions(tokens_most, size = 2:5, min_count = 2)
tokens_most_dict <- quanteda::dictionary(list(custom = c("hat hackers", 
                                                         "cybersecurity education", 
                                                         "white hat",
                                                         "hands on",
                                                         "cyber ethics",
                                                         "types of hackers",
                                                         "meta verse")))

tokens_least_collocations <- detect_multi_word_expressions(tokens_least, size = 2:5, min_count = 2)
tokens_least_dict <- quanteda::dictionary(list(custom = c("video games")))

tokens_overall_collocations <- detect_multi_word_expressions(tokens_overall, size = 2:5, min_count = 2)
tokens_overall_dict <- quanteda::dictionary(list(custom = c("get hacked")))
```

#### Process Tokens With Compound Words

```{r, eval=FALSE}
#| code-fold: false
compound_tokens <- function(tokens, dict) {
  quanteda::tokens_compound(
    tokens,
    pattern = dict,
    concatenator = "_",
    valuetype = "glob",
    window = 0,
    case_insensitive = TRUE,
    join = TRUE,
    keep_unigrams = FALSE,
    verbose = TRUE
  )
}

tokens_most_compound <- compound_tokens(tokens_most, tokens_most_dict)
tokens_least_compound <- compound_tokens(tokens_least, tokens_least_dict)
tokens_overall_compound <- compound_tokens(tokens_overall, tokens_most_dict)
```

#### Word Frequency

```{r, eval=FALSE}
#| code-fold: false
dfm_object_most <- dfm(tokens_most_compound)
dfm_object_least <- dfm(tokens_least_compound)
dfm_object_overall <- dfm(tokens_overall_compound)

word_frequency_most_plot <- plot_word_frequency(dfm_object_most, n = 20)
word_frequency_least_plot <- plot_word_frequency(dfm_object_least, n = 20)
word_frequency_overall_plot <- plot_word_frequency(dfm_object_overall, n = 20)
```

#### Remove Predefined Stopwords

```{r, eval=FALSE}
#| code-fold: false
stopwords <- stopwords("en", source = "snowball")
toks_most_removed <- tokens_remove(tokens_most_compound, pattern = stopwords, verbose = FALSE)
dfm_init_most <- dfm(toks_most_removed)
plot_word_frequency(dfm_init_most, n = 40)
```

```{r, eval=FALSE}
#| code-fold: false
toks_least_removed <- tokens_remove(tokens_least_compound, pattern = stopwords, verbose = FALSE)
dfm_init_least <- dfm(toks_least_removed)
plot_word_frequency(dfm_init_least, n = 20)
```

```{r, eval=FALSE}
#| code-fold: false
toks_overall_removed <- tokens_remove(tokens_overall_compound, pattern = stopwords, verbose = FALSE)
dfm_init_overall <- dfm(toks_overall_removed)
plot_word_frequency(dfm_init_overall, n = 20)
```

#### Remove Common Words in the Dataset

```{r, eval=FALSE}
#| code-fold: false
common_words <- c("also", "with", "can", "just", "really", "able", "get", "got", "unit", "much", "due")

toks_most_removed_common <- tokens_remove(toks_most_removed, pattern = common_words, verbose = FALSE)
dfm_removed_most <- dfm(toks_most_removed_common)
plot_word_frequency(dfm_removed_most, n = 20)
```

```{r, eval=FALSE}
#| code-fold: false
toks_least_removed_common <- tokens_remove(toks_least_removed, pattern = common_words, verbose = FALSE)
dfm_removed_least <- dfm(toks_least_removed_common)
plot_word_frequency(dfm_removed_least, n = 20)
```

```{r, eval=FALSE}
#| code-fold: false
toks_overall_removed_common <- tokens_remove(toks_overall_removed, pattern = common_words, verbose = FALSE)
dfm_removed_overall <- dfm(toks_overall_removed_common)
plot_word_frequency(dfm_removed_overall, n = 20)
```

#### Lemmatize Tokens

##### Word frequency regarding the most preferred topics 

```{r, eval=FALSE}
#| code-fold: false
texts_most <- sapply(toks_most_removed_common, paste, collapse = " ")
parsed_most <- spacy_parse(x = texts_most, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_most_lemmatized <- as.tokens(parsed_most, use_lemma = TRUE)
dfm_most <- dfm(toks_most_lemmatized)
dfm_most_freq <- plot_word_frequency(dfm_most, n = 20)
```

```{r, echo=FALSE}
dfm_most_freq
```

##### Word frequency regarding the least preferred topics 

```{r, eval=FALSE}
#| code-fold: false
texts_least <- sapply(toks_least_removed_common, paste, collapse = " ")
parsed_least <- spacy_parse(x = texts_least, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_least_lemmatized <- as.tokens(parsed_least, use_lemma = TRUE)
dfm_least <- dfm(toks_least_lemmatized)
dfm_least_freq <- plot_word_frequency(dfm_least, n = 20)
```

```{r, echo=FALSE}
dfm_least_freq
```

##### Word frequency regarding overall experiences 

```{r, eval=FALSE}
#| code-fold: false
texts_overall <- sapply(toks_overall_removed_common, paste, collapse = " ")
parsed_overall <- spacy_parse(x = texts_overall, lemma = TRUE, entity = FALSE, pos = FALSE)
toks_overall_lemmatized <- as.tokens(parsed_overall, use_lemma = TRUE)
dfm_overall <- dfm(toks_overall_lemmatized)
dfm_overall_freq <- plot_word_frequency(dfm_overall, n = 20)
```

```{r, echo=FALSE}
dfm_overall_freq
```

## Word Correlation Networks

#### Analyze and Visualize Word Correlation Networks

```{r, eval=FALSE}
#| code-fold: false
word_corr_most <- word_correlation_network(
    dfm_most,
    common_term_n = 2,
    corr_n = 0.32,
    top_node_n = 34,
    nrows = 1,
    height = 1200,
    width = 1000)

word_corr_most_plot <- word_corr_most$plot
word_corr_most_table <- word_corr_most$table
word_corr_most_summary <- word_corr_most$summary
```

[Interactive word correlation networks of the most preferred topics](https://cybersecurity-basic.netlify.app/most)

```{r, echo=FALSE}
word_corr_most_plot
```

```{r, eval=FALSE, echo=FALSE}
htmlwidgets::saveWidget(word_corr_most_plot, "data/most.html", selfcontained = TRUE)
```

```{r, echo=FALSE}
word_corr_most_table
```

```{r, echo=FALSE}
word_corr_most_summary
```

```{r, eval=FALSE}
#| code-fold: false
word_corr_least <- word_correlation_network(
    dfm_least,
    common_term_n = 2,
    corr_n = 0.32,
    top_node_n = 34,
    nrows = 1,
    height = 1100,
    width = 950)

word_corr_least_plot <- word_corr_least$plot
word_corr_least_table <- word_corr_least$table
word_corr_least_summary <- word_corr_least$summary
```


[Interactive word correlation networks of the least preferred topics](https://cybersecurity-basic.netlify.app/least)

```{r, echo=FALSE}
word_corr_least_plot
```

```{r, eval=FALSE, echo=FALSE}
htmlwidgets::saveWidget(word_corr_least_plot, "data/least.html", selfcontained = TRUE)
```

```{r, echo=FALSE}
word_corr_least_table
```

```{r, echo=FALSE}
word_corr_least_summary
```

```{r, eval=FALSE}
#| code-fold: false
word_corr_overall <- word_correlation_network(
    dfm_overall,
    common_term_n = 3,
    corr_n = 0.38,
    top_node_n = 34,
    nrows = 1,
    height = 1100,
    width = 1000)

word_corr_overall_plot <- word_corr_overall$plot
word_corr_overall_table <- word_corr_overall$table
word_corr_overall_summary <- word_corr_overall$summary
```

[Interactive word correlation networks for overall experiences](https://cybersecurity-basic.netlify.app/overall)

```{r, echo=FALSE}
word_corr_overall_plot 
```

```{r, eval=FALSE, echo=FALSE}
htmlwidgets::saveWidget(word_corr_overall_plot, "data/overall.html", selfcontained = TRUE)
```

```{r, echo=FALSE}
word_corr_overall_table
```

```{r, echo=FALSE}
word_corr_overall_summary
```

```{r, eval=FALSE, echo=FALSE}
# print(ls())

list <- c("demo_tbl_ft", "priors", "model", "fixed_coefficient", "pp_check_plot")

save(list = list, file = "data/cybersecurity_clmm.RData")
```


```{r, eval=FALSE, echo=FALSE}
list <- c("dfm_most", "dfm_least", "dfm_overall", 
          "dfm_most_freq", "dfm_least_freq", "dfm_overall_freq",
          "word_corr_most", "word_corr_least", "word_corr_overall",
          "word_corr_most_plot", "word_corr_most_table", "word_corr_most_summary",
          "word_corr_least_plot", "word_corr_least_table", "word_corr_least_summary",
          "word_corr_overall_plot", "word_corr_overall_table", "word_corr_overall_summary")

save(list = list, file = "data/cybersecurity_network.RData")
```

:::

